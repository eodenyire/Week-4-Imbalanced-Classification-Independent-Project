# -*- coding: utf-8 -*-
"""Week 4 Imbalanced Classification Independent Project - Emmanuel Odenyire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UF9LXanJ87lxejA8jKggVaXjKKVmG4uQ

# Week 4 Imbalanced Classification Independent Project

## 1. Defining the Question

### a) Specifying the Data Analysis Question

We are needed to predict whether a customer in Beta Bank will leave the bank soon or not.

### b) Understanding the Context

Beta Bank customers are leaving: little by little, chipping away every month. The bankers figured out it’s cheaper to save the existing customers rather than to attract new ones.
We need to predict whether a customer will leave the bank soon. You have the data on clients’ past behavior and termination of contracts with the bank.
Build a model with the maximum possible F1 score. To pass the project, you need an F1 score of at least 0.59. Check the F1 for the test set.
Additionally, measure the AUC-ROC metric and compare it with the F1.

1. Download and prepare the data. Explain the procedure.
2. Examine the balance of classes. Train the model without taking into account the
imbalance. Briefly describe your findings.
3. Improve the quality of the model. Make sure you use at least two approaches to
fixing class imbalance. Use the training set to pick the best parameters. Train different models on training and validation sets. Find the best one. Briefly describe your findings.
4. Perform the final testing.

### c) Recording the Experimental Design in this analysis

● Data Importation

● Data Exploration

● Data Preparation

● Data Modeling (Using Decision Trees, Random Forest and Logistic Regression)

● Model Evaluation

● Hyparameter Tuning

● Findings and Recommendations

### d) Metric of success

Designing a model that will predict whether a customer will leave bank with an F1 score of at least 0.59

#Data Importation

##Loading the necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from joblib import dump
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.utils import resample
from sklearn.metrics import roc_auc_score
from sklearn.metrics import make_scorer
from sklearn.utils import shuffle
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import classification_report
# %matplotlib inline

"""## Loading data from the dataset"""

# Loading the dataset using pd.read_csv() function
betabank_df = pd.read_csv('https://bit.ly/2XZK7Bo')
betabank_df.head()

"""Observation: Three columns have boolean values, HasCrCard, IsActiveMember and Exited columns. Others have either text or numeric values"""

#Checking additional data information using .info() fucntion
betabank_df.info()

"""Observation:- We have a mixture of integer, object and float data types

# Data Exploration: Exploratory Data Analysis (EDA)
"""

#Checking for nulls
betabank_df.isnull().sum()

"""We will use Exited column as our target. For this analysis some of the columns i.e RowNumber, CustomerId and Surname will be remove as they aren't useful features. For the nulls in Tenure, will replace them with the mean.

## Data cleaning
"""

#Replace nulls in tenure with mean
betabank_df['Tenure']= df['Tenure'].fillna(df['Tenure'].mean())
#Convert float datatype in tenure to int
betabank_df['Tenure'] = df['Tenure'].astype(int)

#Check the data again using .info() function
betabank_df.info()

"""Observation: Float datatype in Tenure column converted successfully to integer"""

#Confirm if we have nulls again
betabank_df.isnull().sum()

"""Observation: - We don't nulls anymore in any of the column

## Data visualization
"""

#Checking the distribution of the target column
sns.pairplot(betabank_df, hue = 'Exited')

sns.histplot(x=betabank_df['Exited'], hue= df['Exited'])

"""There are more customers who haven't left. This might affect model biasness"""

#Getting the co-relation
betabank_df.corr()

#Plotting the heat map of the correlations
sns.heatmap(betabank_df.corr(),annot=True, cmap='Reds', linecolor='Green', linewidths=1.5)

"""# Data preparation

Setting the targets and features ready for the models
"""

#One hot encoding
df = pd.get_dummies(betabank_df, drop_first=True)

#Creating dataframes: Features and target dataframes
features = betabank_df.drop(columns=['Exited', 'RowNumber','Gender', 'CustomerId','Surname', 'Geography'])
target =  betabank_df['Exited']

# Splitting the data inot four variables
features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.20, random_state=12345)
features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, test_size=0.2, random_state=12345 )

# Let us try normalizing the data now
to_normalize = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Tenure', 'NumOfProducts']
scaler = StandardScaler()
scaler.fit(features_train[to_normalize])
features_train.loc[:,to_normalize] = scaler.transform(features_train[to_normalize])
features_valid.loc[:,to_normalize] = scaler.transform(features_valid[to_normalize])
features_test.loc[:,to_normalize] = scaler.transform(features_test[to_normalize])

"""Note that, one hot encoding was required to convert the categorical variable to numeric. Allowing the 'inverse' of each data type will damage our outcomes since there will be high correlation between them. It makes sense to 'drop first' and only let one of them remain. Similarly, we don't need France, Germany and Spain coulmns since a row not being 1 for Germany and Spain implies that it is France. After splitting, I checked to see if the splits make sense, and they do. The sample size is also good.

Afterwards, normalize the numeric columns

# Data Modelling : Machine Learning Modelling, Evaluation and Hyparameter Tuning

## Without taking into account the imbalance
"""

#  Looking at the class imbalance:
print(betabank_df[betabank_df['Exited'] == 1]['Exited'].count())
print(betabank_df[betabank_df['Exited'] == 0]['Exited'].count())

"""There is an evident mbalance, approximately a ration of 1:4.

### Logistic Regression
"""

#Assuming theclass imbalance:
LogRegModImb = LogisticRegression(solver='liblinear', random_state=12345)
LogRegModImb.fit(features_train,target_train)
print('Accuracy', LogRegModImb.score(features_valid, target_valid))
print('f1 score:' ,f1_score(target_valid, LogRegModImb.predict(features_valid)))
print('AUC:', roc_auc_score(target_valid, LogRegModImb.predict_proba(features_valid)[:,1]))

features_train.head()

"""We have a accuracy of 0.81, f1 score of 0.31, and AUC of 0.76 when we do not account for imbalance and use logistic regression. We don't need to check Random Forest and Decision Tree because if the imbalance affects the results of Logistic Regression, it will naturally affect the results of Random Forest and Decision Tree since they do not perform well on imbalanced data sets.

## Taking into account the imbalance

### Logistic Regression
"""

#Upsampling function
def upsample(features, target, repeat):
    features_zeros = features[target == 0]
    features_ones = features[target == 1]
    target_zeros = target[target == 0]
    target_ones = target[target == 1]

    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)
    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)
    
    features_upsampled, target_upsampled = shuffle(
        features_upsampled, target_upsampled, random_state=12345)
    
    return features_upsampled, target_upsampled

features_upsampled, target_upsampled = upsample(features_train, target_train, 10)

upsampled_LogReg_model =LogisticRegression(random_state=12345,solver='liblinear')
upsampled_LogReg_model.fit(features_upsampled, target_upsampled)
upsampled_LogReg_predicted_valid = upsampled_LogReg_model.predict(features_valid)

print('Accuracy', upsampled_LogReg_model.score(features_valid, target_valid))
print('f1 score:' ,f1_score(target_valid, upsampled_LogReg_predicted_valid))
print('AUC:',roc_auc_score(target_valid, upsampled_LogReg_model.predict_proba(features_valid)[:,1]))

"""Upsampling makes our f1 score 0.393. This isn't as good as the in built balanced feature of the Logisitic regression model."""

#Downsampling function
def downsample(features, target, fraction):
    features_zeros = features[target == 0]
    features_ones = features[target == 1]
    target_zeros = target[target == 0]
    target_ones = target[target == 1]

    features_downsampled = pd.concat(
        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])
    target_downsampled = pd.concat(
        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])
    
    features_downsampled, target_downsampled = shuffle(
        features_downsampled, target_downsampled, random_state=12345)
    
    return features_downsampled, target_downsampled

features_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)

down_LogReg_model =LogisticRegression(random_state=12345,solver='liblinear')
down_LogReg_model.fit(features_downsampled, target_downsampled)
down_LogReg_predicted_valid = down_LogReg_model.predict(features_valid)

print("F1:", f1_score(target_valid, down_LogReg_predicted_valid))
print('Accuracy:', down_LogReg_model.score(features_valid, target_valid))
print("AUC-ROC:", roc_auc_score(target_valid, down_LogReg_model.predict_proba(features_valid)[:,1]))

"""Downsampling makes our f1 score 0.391. This isn't as good as the in built balanced feature of the Logisitic regression model and is almost the same as upsampling.

## Decision Tree
"""

depth_param = {'max_depth':range(1,25)}
DecTreeMod = DecisionTreeClassifier(random_state=12345)
DecTreeModOpt = GridSearchCV(DecTreeMod,depth_param)
DecTreeModOpt.fit(features_train, target_train)
print(DecTreeModOpt.best_estimator_)
DecTreeModOpt_predicted_valid = DecTreeModOpt.predict(features_valid)
print("F1:", f1_score(target_valid, DecTreeModOpt_predicted_valid))
print('Accuracy:', DecTreeModOpt.score(features_valid, target_valid))
print("AUC-ROC:", roc_auc_score(target_valid, DecTreeModOpt.predict_proba(features_valid)[:,1]))

"""Just from using decision trees with optimized hyper parameters, our f1 score 0.51. This is better than the in built balanced feature of the Logisitic regression model, but not by much."""

# Using the optimal max depth
decision_classifier = DecisionTreeClassifier(max_depth = 7, random_state = 12345)
decision_classifier.fit(features_train, target_train)
score = decision_classifier.score(features_test, target_test)
print('Score: {}'.format(score))

"""*   Random Forest Classifier had the best accuracy score of 80.87%

*   Decision Tree Classifier had the second best accuracy score of 79.78%

*   Logic Regression had the least accuracy score of 76.51%

### Random Forest
"""

depth_param = {'max_depth':range(1,10), 'n_estimators':range(1,50)}
RandForestMod = RandomForestClassifier(random_state=12345)
RandForestOpt = GridSearchCV(RandForestMod,depth_param)
RandForestOpt.fit(features_train, target_train)
print(RandForestOpt.best_estimator_)
RandForestOpt_predicted_valid = RandForestOpt.predict(features_valid)
print("F1:", f1_score(target_valid, RandForestOpt_predicted_valid))
print('Accuracy', RandForestOpt.score(features_valid, target_valid))
print("AUC-ROC:", roc_auc_score(target_valid, RandForestOpt.predict_proba(features_valid)[:,1]))

"""Random forests gives us a f1 score of 0.54, although it took a very long time to run this chunk of code. If run time is a priority, the parameter space needs to be greatly reduced. However, we have come across an issue. We need an f1 score of at least 0.59, and we have already done an exhaustive search over a huge parameter space. Let us try to keep one parameter constant and increase the range of the other parameter to see if that can help us improve our score. For the above model, max_depth of 8 gave us the best result. Let's keep that constant and increase the range of n_estimators and try again. Most importantly, let us add the argument: 'class weight = balanced' since simply increasing the parameter space alone is most likely not going to increase our f1 score by so much."""

depth_param = {'n_estimators':range(1,200)}
RandForestMod = RandomForestClassifier(random_state=12345, max_depth = 8,class_weight='balanced')
RandForestOpt = GridSearchCV(RandForestMod, depth_param)
RandForestOpt.fit(features_train, target_train)
print(RandForestOpt.best_estimator_)
RandForestOpt_predicted_valid = RandForestOpt.predict(features_valid)
print("F1:", f1_score(target_valid, RandForestOpt_predicted_valid))
print('Accuracy:', RandForestOpt.score(features_valid, target_valid))
print("AUC-ROC:", roc_auc_score(target_valid, RandForestOpt.predict_proba(features_valid)[:,1]))

"""We get a f1 score of 0.604 and the n_estimators we need is 190. Finally, we have achieved an acceptable number gives us enough confidence to take our model to the testing data. Max depth is 8 since we specified that.

## Evaluate the best model: random forest
"""

RandForestOpt_predicted_test = RandForestOpt.predict(features_test)
print("F1:", f1_score(target_test, RandForestOpt_predicted_test))
print("AUC-ROC:", roc_auc_score(target_test, RandForestOpt.predict_proba(features_test)[:,1]))
print('Accuracy:', RandForestOpt.score(features_valid, target_valid))

print(classification_report(target_test, RandForestOpt_predicted_test))

# Let's plot the AUC-ROC curve.
probabilities_valid = RandForestOpt.predict_proba(features_valid)
probabilities_one_valid = probabilities_valid[:, 1]

fpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid)

plt.figure()
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')
plt.show()

"""We see a deviation from the random model (orange line) that signifies that our model performs that much better than someone just working by chance. This is validated by the AUC we calculated."""

# Let's plot the precision-recall curve
precision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])

plt.figure(figsize=(6, 6))
plt.step(recall, precision, where='post')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Precision-Recall Curve')
plt.show()

"""# Findings and Recommendations

## Findings

* The best model is a Random Forest Classifier. Bootstrap is set to true and the  class_weight parameter is set to ' balanced '. Setting it to balanced is very important otherwise the performance expectations are not met.

* The hyperparameters max_depth is set to 8 and n_estimators is set to 190. A random_state = 12345 will give you the exact results I produced above.

* For the test dataset, the F1 score is 0.64 and the AUC-ROC score is 0.86. Both these metrics signify good quality and meet the expectations of the assignment.

## Recommendations

*   Use of Random Forest model is recommended
*   More training data with less bias will increase the accuracy of the model
*   Hyper paramter tuning is required in order to improve model accuracy.
"""